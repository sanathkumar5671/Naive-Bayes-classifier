{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NAME: Sanathkumar Adavayya Swamy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format\n",
    "import pandas as pd\n",
    "def load_data():\n",
    "    dataset = pd.read_csv('student.csv')\n",
    "    label_p = dataset[\"Grade\"] #Label is the value or a target which I have to aim to predict in this classifier\n",
    "    features_p = dataset.iloc[:,0:29] #Features of the dataset is split with .iloc function in Pandas\n",
    "    \n",
    "    #Converting the string values to numberic with refernce to their frequency \n",
    "    \n",
    "    numeric_features_p = features_p.apply(lambda feature: pd.factorize(feature)[0])\n",
    "    numeric_labels_p = pd.factorize(label_p)[0]\n",
    "    dataset.iloc[:,0:29] = numeric_features_p\n",
    "    dataset[\"Grade\"] = numeric_labels_p\n",
    "    \n",
    "    #the Data are converted numeric values accroding to their occurence in the dataset\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should split a data set into a training set and hold-out test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(t_data):\n",
    "    # The features and values are split into X=features y=values(\"Grade\")\n",
    "    X = load_data() \n",
    "    X_train, X_test = train_test_split(X,test_size=0.2,random_state = 0)\n",
    "    if t_data == 'X_train':\n",
    "        return X_train\n",
    "    elif t_data == 'X_test':\n",
    "        return X_test\n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model     \n",
    "def train(X):\n",
    "    #the ds variable is stroing all the columns\n",
    "    ds = X.iloc[:,0:29].columns\n",
    "    ds_val =ds.values.tolist()\n",
    "    df = []\n",
    "    lis_val = []\n",
    "    liklihood_val = []\n",
    "    \n",
    "    #As I know that there are 6 different class function in our lable.\n",
    "    #I am going to count the total values of each class present in the label\n",
    "    \n",
    "    for x in range(6):\n",
    "        k = X[X['Grade'] == x].count()\n",
    "        k_val = k.values.tolist()\n",
    "        lis_val.append(k_val[x])\n",
    "    \n",
    "    # Now I know the prior values of the lable and there count as well. \n",
    "    #Now I can find the Liklihood of the values by counting the values present in each attributes and getting \n",
    "    #the correct Grade value and the attribute values  \n",
    "    \n",
    "    \n",
    "    for x in range(len(ds_val)):\n",
    "        \n",
    "        #alpha and M_val are used for laplace smoothing \n",
    "        \n",
    "        M_val = len(set(X[ds_val[x]].values))\n",
    "        alpha = 1 \n",
    "        for m in range(len(set(X[ds_val[x]].values))):\n",
    "            for y in range(6):\n",
    "                df = X[(X['Grade'].values == y) & (X[ds_val[x]].values == m)].count()\n",
    "                df_val = df.values.tolist()\n",
    "                \n",
    "                #Laplace Smoothing is applied to the values alpha and M_val, where alpha is 1 and M_val is the how much x attribute values can be taken\n",
    "                \n",
    "                val = float((alpha + df_val[y])/(M_val * lis_val[y]))\n",
    "                liklihood_val.append([ds_val[x],m, y , val])\n",
    "                # finally the liklihood values are returned from the fuction in a list of lists\n",
    "    return liklihood_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since train() function takes longer time to execte, I have stored these values in a list and I have stored the priors in a seperate list outside the train function to make it more useable \n",
    "train_val = train(split_data('X_train'))\n",
    "lis_grade_count_val = []\n",
    "priors_val = []\n",
    "for x in range(6):\n",
    "        grade_count = split_data('X_train')[split_data('X_train')['Grade'] == x].count()\n",
    "        grade_count_val = grade_count.values.tolist()\n",
    "        lis_grade_count_val.append(grade_count_val[x])\n",
    "for y in range(len(lis_grade_count_val)):\n",
    "    priors_val.append(lis_grade_count_val[y]/sum(lis_grade_count_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(Y):\n",
    "    ls = Y.iloc[:,0:29].columns\n",
    "    col_val = ls.values.tolist()\n",
    "    lis_col_val = []\n",
    "    lis_train_val = []\n",
    "    fm = Y.iloc[:,0:29]\n",
    "    fm_val = fm.values.tolist()\n",
    "    test_values = []\n",
    "    \n",
    "    #It would be easier to compare the values in different list according to classess\n",
    "    \n",
    "    grade_0_lis = []\n",
    "    grade_1_lis = []\n",
    "    grade_2_lis = []\n",
    "    grade_3_lis = []\n",
    "    grade_4_lis = []\n",
    "    grade_5_lis = []\n",
    "    \n",
    "    #predicting the test data\n",
    "    \n",
    "    for z in range(6):\n",
    "        for x in range(len(fm_val)):\n",
    "            mul_1_val = 1.0\n",
    "            for y in range(len(col_val)):\n",
    "                for m in range(len(train_val)):\n",
    "                    if (col_val[y] == train_val[m][0]) & (fm_val[x][y] == train_val[m][1]) & (train_val[m][2] == z):\n",
    "                        mul_1_val = mul_1_val * train_val[m][3]\n",
    "            test_values.append([z,mul_1_val *  priors_val[z]])\n",
    "    max_lis = []\n",
    "    \n",
    "    #Now I can store this predicted data according to each classes for comparision \n",
    "    \n",
    "    for x in range(len(test_values)):\n",
    "        if (test_values[x][0] == 0):\n",
    "            grade_0_lis.append(test_values[x][1])\n",
    "        elif (test_values[x][0] == 1):\n",
    "            grade_1_lis.append(test_values[x][1])\n",
    "        elif (test_values[x][0] == 2):\n",
    "            grade_2_lis.append(test_values[x][1])\n",
    "        elif (test_values[x][0] == 3):\n",
    "            grade_3_lis.append(test_values[x][1])\n",
    "        elif (test_values[x][0] == 4):\n",
    "            grade_4_lis.append(test_values[x][1])\n",
    "        elif (test_values[x][0] == 5):\n",
    "            grade_5_lis.append(test_values[x][1])\n",
    "\n",
    "            # comparing the data in each list accoring to there highest values and storing them in a list\n",
    "            \n",
    "    for y in range(len(grade_0_lis)):\n",
    "        #max_val stores the max value from all the list which are stored in the Grade for each row\n",
    "        max_val = max(grade_0_lis[y], grade_1_lis[y],grade_2_lis[y],grade_3_lis[y],grade_4_lis[y],grade_5_lis[y])\n",
    "        if (max_val  == grade_0_lis[y]):\n",
    "            max_lis.append(0)\n",
    "        elif (max_val  == grade_1_lis[y]):\n",
    "            max_lis.append(1)\n",
    "        elif (max_val  == grade_2_lis[y]):\n",
    "            max_lis.append(2)\n",
    "        elif (max_val  == grade_3_lis[y]):\n",
    "            max_lis.append(3)\n",
    "        elif (max_val  == grade_4_lis[y]):\n",
    "            max_lis.append(4)\n",
    "        elif (max_val  == grade_5_lis[y]):\n",
    "            max_lis.append(5)\n",
    "            \n",
    "# max_lis returns the predicted grade of the test data\n",
    "\n",
    "    return max_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions in terms of accuracy\n",
    "def evaluate(Y):\n",
    "    pre_test_val = predict(Y)\n",
    "    actuall_test_val = []\n",
    "    actuall_test = Y['Grade']\n",
    "    actuall_test_val = actuall_test.values.tolist()\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    lis_pos = []\n",
    "    lis_neg = []\n",
    "    for x in range(len(actuall_test_val)):\n",
    "        if (actuall_test_val[x] == pre_test_val[x]):\n",
    "            pos = pos + 1\n",
    "            lis_pos.append(x)\n",
    "        else:\n",
    "            neg = neg +1\n",
    "            lis_neg.append(x)\n",
    "    val = set(Y.count())\n",
    "    lis_val = list(val)\n",
    "    accuracy = (pos/lis_val[0]) * 100\n",
    "    return (round(accuracy,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.231"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(split_data('X_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQMAAAEWCAYAAABiyvLjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAT7klEQVR4nO3deZRcZZ3G8e9DAtIIiAkRgWQMowGGcQ5EIqPiuAAiqCAjR4RRWYY5Oe7gOu6C4zZuw+DCEQQR2UQBD6KAKJtgQBoImwFlFIcM4aRNBBJAMfDMH/ftWOlUdarTfesWzfM5pw5V996+7+826afeu71XtomI2KDpAiKiPyQMIgJIGEREkTCICCBhEBFFwiAigIRBRBQJgxpI+hdJg5JWSloi6SJJL26Zv5OkCyQ9IGmFpMslvahl/mxJlvSjEes9XdIx5f3LJD1e2hh+/bDMO1XSp0b87PA6p5bPL5b0i1LDcknXSHp+mXe4pKtH/Pzhkm6V9LCk+ySdIGmLlvnHlPW/vmXa1DJtdoff0xVl/s4jpv+gTH9Zmxos6aCWaW9s2f5HRv5OyjJ3l3krS+2nStq0ZR2rf1+S5pbfyXNa5u8q6f5O2zFZJAwmmKT3AMcBnwG2Av4G+Drw2jL/2cA1wK3AdsA2wPnATyS9cMTqXiBp91Gau9f2pi2v/bqscXPgQuArwDRgW+BY4M8dln8v8J/A+4GnAS8AngVcKmmjlkWXA5+UNKWbOopfA4e2tDW9rH+ozbKHlTYOG55g+4zh7Qf2ZcTvpOVn9yufdwHmAh9qV4ztm4CvASepsiFwCvBx23ePYbuecBIGE0jS04BPAm+3fZ7th2z/xfYPbb+/LHYMsMD2R2wvt73C9vHAd6j+4Fp9HvgUE297ANtn2X7M9iO2f2L7ljbbtDlVULzT9sVle+4GDqIKhDe1LH4x8OiIaetyBvCGlgA5hCocHx1Rx7OAlwLzgVdK2moMbaxm+z7gEqpQ6ORYYOvS1oeBlcBX16e9J5KEwcR6IbAx1T/mTl4BfK/N9HOA3SVt0jLta8D2kvaauBKB6tv4MUnflrSvpKePsuyLqLbpvNaJtlcCF1Ftz+rJwMeAT5Rv1G7cC/wK2Lt8PhQ4rc1yhwKDts8FFgFv7HL9a5A0k6oHcVenZWz/GTiSKpzfCxxp+/H1ae+JJGEwsaYDf7C9apRltgSWtJm+hOr/R+sf5p+AT9O5d7BN2Zcdfh3UYbk12H4QeDHVH+9JwFA5htHu23ZLOm/TkjK/dd0XUHXx/62bWorTgEMl7QBsYXtBm2UOBc4s78+kZVehSz+QtAK4B1gKfGIdy98GrAJutX3HGNt6QkoYTKxlwJbDB+k6+ANVF3SkrYHHgT+OmH4SsJWkdscD7rW9RcvrnDJ9FTDym3nDsv7HAWwvsn247ZnAc6mOXRzXod5O27R1mT/SR4GPUPUounEesAfwTqrdpTWU4ybbAWeXSWcC/yBptK7+SAfY3gx4GbAjI0KsjS8BVwIzJR08hnaesBIGE2sB1bf5AaMs81Pg9W2mH0R1LOHh1om2/0K1D/sfgLqs43+B2SOmbQfc0667W775TqUKhZEWUB1YfF3rRElPpepu/6zN+i6l6oa/rZtiyzZfBLyVNmFA1QsQsFDSfcB1ZfqhbZZdV1tXUm3rFzstI2lPqgO+bymv/5Y0baxtPdEkDCaQ7QeAjwNfk3SApE0kbVj2yz9fFjsWeJGkT0uaJmkzSe+k+of97x1W/R3gKcA+XZZyLvBqSXtLmiJpG6pv67MBJO0o6b1l/xlJs6gO3F3bYZuOBb4iaZ+yPbOpjnsspv0fL1Q9gw90WS9UB+peOvKIvaSNqYJyPtVBv+HXO4E3rqMX1slxwCva9SxKyJ0EHG17yPZFwKXAf61HO08oCYMJZvvLwHuo/viGqPZR3wH8oMz/DdX++s7A3VT73QcCr7R9TYd1Pka1j9vVt5Pt26n+uD9LdSpuAdW36bFlkRXAPwLXSXqIKgRuozpY1m59n6f6Y/0i8GBZ1z3AnuVgW7ufuQb4ZTf1luXvtX11m1kHAI8Ap9m+b/gFnAxMofuAbG1riOo4xcfazP4McIftM1qmHQ3sK2nvNstPGsrgJhEB6RlERJEwiAggYRARRcIgIgBYn9MytZk+fbpnzZrVdBl9bYMNkt+jeeCBB5ouYS0bbbTRuhfqkSVLlnD//fe3vV6lr8Jg1qxZXHbZZU2X0dcGBgaaLqGvXXjhhU2XsJZ++oI74ogjOs7L10xEAAmDiCgSBhEBJAwiokgYRASQMIiIImEQEUDCICKKhEFEAAmDiCgSBhEBJAwiokgYRARQcxiU0XTvlHSXpA/W2VZEjE9tYVCenfc1qrH1dwIOkbRTXe1FxPjU2TPYDbjL9m9tP0o1Zv9ra2wvIsahzjDYlmps/WGLy7Q1SJovaVDS4LJly2osJyJGU2cYtBtaaa2HNNg+0fY82/OmT59eYzkRMZo6w2Ax0Dre00yqx29HRB+qMwyuB+ZI2k7SRsDBwAU1thcR41DbgKi2V0l6B3AJ1TPxTinPAIyIPlTr6Mi2fwz8uM42ImJi5ArEiAASBhFRJAwiAkgYRESRMIgIIGEQEUXCICKAhEFEFAmDiAASBhFRJAwiAkgYRERR641KYzV16lSmTZvWdBmr3XzzzU2XsJYFCxY0XcIaDjrooKZLWEO/1QPw8MMPN13CagMDAx3npWcQEUDCICKKhEFEAAmDiCgSBhEBJAwiokgYRASQMIiIImEQEUDCICKKhEFEAAmDiCgSBhEBJAwiokgYRARQYxhIOkXSUkm31dVGREycOnsGpwL71Lj+iJhAtYWB7auA5XWtPyImVuPHDCTNlzQoaXBoaKjpciKetBoPA9sn2p5ne96MGTOaLifiSavxMIiI/pAwiAig3lOLZwELgB0kLZZ0ZF1tRcT41fbcBNuH1LXuiJh42U2ICCBhEBFFwiAigIRBRBQJg4gAEgYRUSQMIgJIGEREkTCICCBhEBFFwiAigIRBRBS13ag0GWy//fZNl7CWBQsWNF3CGqZNm9Z0CWs44YQTmi5hLQMDA02XsNoGG3T+/k/PICKAhEFEFAmDiAASBhFRJAwiAkgYRESRMIgIIGEQEUXCICKAhEFEFAmDiAASBhFRJAwiAkgYRERR54NXZ0m6XNIiSbdLOqqutiJi/Oocz2AV8F7bN0raDLhB0qW2f1VjmxGxnmrrGdheYvvG8n4FsAjYtq72ImJ8enLMQNJsYC5wXZt58yUNShocGhrqRTkR0UbtYSBpU+Bc4GjbD46cb/tE2/Nsz5sxY0bd5UREB7WGgaQNqYLgDNvn1dlWRIxPnWcTBJwMLLL95braiYiJ0fFsgqSvAO403/a71rHu3YE3A7dKWlimfdj2j8dcZUTUbrRTi4PjWbHtqwGNZx0R0Tsdw8D2t3tZSEQ0a7TdhB8y+m7C/rVUFBGNGG034Ys9qyIiGjfabsKVvSwkIpq1znsTJM0BPgvsBGw8PN3239ZYV0T0WDfXGXwLOIHqxqOXA6cB36mzqIjovW7CYMD2zwDZ/r3tY4A96i0rInqtm1uY/yRpA+A3kt4B/B/wjHrLiohe66ZncDSwCfAuYFeqqwoPq7OoiOi9dfYMbF9f3q4Ejqi3nIhoSjdnEy6nzcVHtnPcIGIS6eaYwfta3m8MHEh1ZiEiJpFudhNuGDHpGkm1XJC0atUqli9fXseq18vAwEDTJazlsMP663DN4sWLmy6h75122mlNl7DasmXLOs7rZjdhWsvHDagOIj5z/GVFRD/pZjfhBqpjBqLaPfgdcGSdRUVE73UTBn9n+0+tEyQ9paZ6IqIh3Vxn8Is20xZMdCER0azRxjN4JtVzDgYkzeWvoxZtTnURUkRMIqPtJrwSOByYCXyJv4bBg8CH6y0rInptXcOefVvSgbbP7WFNEdGAbo4Z7Cppi+EPkp4u6VM11hQRDegmDPa1ff/wB9t/BF5VX0kR0YRuwmBK66lESQNATi1GTDLdXGdwOvAzSd8qn48AMox6xCTTzb0Jn5d0C7AX1RmFi4Fn1V1YRPRWt89avA94nOqOxT2BRbVVFBGNGO2io+2Bg4FDgGXAd6nGQXx5j2qLiB4abTfhDuDnwH627wKQ9O5uVyxpY+AqqoONU4Hv2/7EOGqNiBqNtptwINXuweWSTpK0J2N7kOqfgT1s7wzsAuwj6QXrX2pE1KljGNg+3/YbgB2BK4B3A1tJOkHS3utasSsry8cNy6vjsxsjolnrPIBo+yHbZ9h+DdV9CguBD3azcklTJC0ElgKX2r6uzTLzJQ1KGhxtFJaIqFe3ZxMAsL3c9je6HQzV9mO2d6EKkd0kPbfNMifanmd73vTp08dSTkRMoDGFwfoqlzNfAezTi/YiYuxqCwNJM4ZvcCqXMO9FdYYiIvpQN5cjr6+tqW6BnkIVOufYvrDG9iJiHGoLA9u3AHPrWn9ETKyeHDOIiP6XMIgIIGEQEUXCICKAhEFEFAmDiAASBhFRJAwiAkgYRESRMIgIIGEQEUXCICKAeu9aHLOpU6cybdq0psuIMZg5c2bTJaxhzz33bLqEtcyZM6fpElY7/vjjO85LzyAigIRBRBQJg4gAEgYRUSQMIgJIGEREkTCICCBhEBFFwiAigIRBRBQJg4gAEgYRUSQMIgJIGEREUXsYSJoi6SZJeehqRB/rRc/gKGBRD9qJiHGoNQwkzQReDXyzznYiYvzq7hkcB3wAeLzTApLmSxqUNDg0NFRzORHRSW1hIOk1wFLbN4y2nO0Tbc+zPW/GjBl1lRMR61Bnz2B3YH9JdwNnA3tIOr3G9iJiHGoLA9sfsj3T9mzgYOAy22+qq72IGJ9cZxARQI+GSrd9BXBFL9qKiPWTnkFEAAmDiCgSBhEBJAwiokgYRASQMIiIImEQEUDCICKKhEFEAAmDiCgSBhEBJAwiokgYRATQo7sWI3rlzjvvbLqEtcyZM6fpErqSnkFEAAmDiCgSBhEBJAwiokgYRASQMIiIImEQEUDCICKKhEFEAAmDiCgSBhEBJAwiokgYRASQMIiIotZbmCXdDawAHgNW2Z5XZ3sRsf56MZ7By23/oQftRMQ4ZDchIoD6w8DATyTdIGl+uwUkzZc0KGlwaGio5nIiopO6w2B3288D9gXeLuklIxewfaLtebbnzZgxo+ZyIqKTWsPA9r3lv0uB84Hd6mwvItZfbWEg6amSNht+D+wN3FZXexExPnWeTdgKOF/ScDtn2r64xvYiYhxqCwPbvwV2rmv9ETGxcmoxIoCEQUQUCYOIABIGEVEkDCICSBhERJEwiAggYRARRcIgIoCEQUQUCYOIABIGEVHIdtM1rCZpCPj9BKxqS6Cfxl1MPaPrt3qg/2qaqHqeZbvtKEJ9FQYTRdJgP43EnHpG12/1QP/V1It6spsQEUDCICKKyRoGJzZdwAipZ3T9Vg/0X0211zMpjxlExNhN1p5BRIxRwiAigEkWBpL2kXSnpLskfbAP6jlF0lJJfTFEvKRZki6XtEjS7ZKOariejSX9UtLNpZ5jm6xnmKQpkm6SdGHTtUD1AGNJt0paKGmwtnYmyzEDSVOAXwOvABYD1wOH2P5VgzW9BFgJnGb7uU3V0VLP1sDWtm8sz7S4ATigqd+RqnH0n2p7paQNgauBo2xf20Q9LXW9B5gHbG77NU3WUuq5G5hX9wOMJ1PPYDfgLtu/tf0ocDbw2iYLsn0VsLzJGlrZXmL7xvJ+BbAI2LbBemx7Zfm4YXk1+u0kaSbwauCbTdbRhMkUBtsC97R8XkyD/9D7naTZwFzguobrmCJpIbAUuNR2o/UAxwEfAB5vuI5W63yA8USYTGGgNtMmxz7QBJO0KXAucLTtB5usxfZjtncBZgK7SWpsd0rSa4Cltm9oqoYO1vkA44kwmcJgMTCr5fNM4N6GaulbZd/8XOAM2+c1Xc8w2/cDVwD7NFjG7sD+ZR/9bGAPSac3WA/QuwcYT6YwuB6YI2k7SRsBBwMXNFxTXykH7E4GFtn+ch/UM0PSFuX9ALAXcEdT9dj+kO2ZtmdT/fu5zPabmqoHevsA40kTBrZXAe8ALqE6MHaO7dubrEnSWcACYAdJiyUd2WQ9VN98b6b6xltYXq9qsJ6tgcsl3UIV5pfa7ovTeX1kK+BqSTcDvwR+VNcDjCfNqcWIGJ9J0zOIiPFJGEQEkDCIiCJhEBFAwiAiioTBk4Ckx8ppxNskfU/SJuNY18uG7+aTtP9od4dK2kLS29ajjWMkvW99a4z1kzB4cnjE9i7lzslHgbe0zlRlzP8WbF9g+3OjLLIFMOYwiGYkDJ58fg48R9LsMq7B14EbgVmS9pa0QNKNpQexKaweJ+IOSVcDrxtekaTDJX21vN9K0vllbIKbJb0I+Bzw7NIr+UJZ7v2Srpd0S+v4BZI+Usai+CmwQ89+G7FawuBJRNJUqptdbi2TdqAaa2Eu8BDwUWCvclPMIPAeSRsDJwH7Af8EPLPD6o8HrrS9M/A84Hbgg8D/lF7J+yXtDcyhurZ+F2BXSS+RtCvV5b9zqcLm+RO86dGFqU0XED0xUG4ThqpncDKwDfD7loFEXgDsBFxT3cLARlSXUu8I/M72bwDKjTvtbqPdAzgUqjsRgQckPX3EMnuX103l86ZU4bAZcL7th0sbuaekAQmDJ4dHym3Cq5U/+IdaJ1HdG3DIiOV2YeJuBRfwWdvfGNHG0RPYRqyn7CbEsGuB3SU9B0DSJpK2p7qLcDtJzy7LHdLh538GvLX87BRJmwMrqL71h10C/GvLsYhtJT0DuAr4Z0kD5Q69/SZ426ILCYMAwPYQcDhwVrmL8FpgR9t/otot+FE5gNjpwbhHAS+XdCvV2Ip/b3sZ1W7HbZK+YPsnwJnAgrLc94HNylBs3wUWUo218PPaNjQ6yl2LEQGkZxARRcIgIoCEQUQUCYOIABIGEVEkDCICSBhERPH/7hWkCH+72aoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.10      0.16        21\n",
      "           1       0.56      0.11      0.18        46\n",
      "           2       0.46      0.48      0.47        23\n",
      "           3       0.50      0.11      0.18        28\n",
      "           4       0.06      1.00      0.11         4\n",
      "           5       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.19       130\n",
      "   macro avg       0.35      0.30      0.18       130\n",
      "weighted avg       0.47      0.19      0.21       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 2 B\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "actuall_test = split_data('X_test')['Grade']\n",
    "actuall_test_val = actuall_test.values.tolist()\n",
    "pre_test_val = predict(split_data('X_test'))\n",
    "\n",
    "#creating a confusion matrix using the sklearn library\n",
    "\n",
    "cf = confusion_matrix(actuall_test_val,pre_test_val)\n",
    "\n",
    "#I am going to plot the matrix values in a graph to understand it better\n",
    "\n",
    "plt.imshow(cf, cmap='binary', interpolation='None')\n",
    "plt.title('CONFUSION MATRIX')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actuall')\n",
    "plt.show()\n",
    "\n",
    "#I am going to retrieve all the information of the predicted and acctual values using the sklearn library\n",
    "\n",
    "print(classification_report(actuall_test_val, pre_test_val, labels=[0, 1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation():\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import KFold\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn import tree\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    X = load_data()\n",
    "    # KFold Cross Validation approach\n",
    "    kf = KFold(n_splits=5,shuffle=False)\n",
    "    kf.split(X)    \n",
    "    accuracy_list = []\n",
    "    # Iterate over each train-test split\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split train-test\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        train(X_train)\n",
    "        train_val = train(X_train)\n",
    "        lis_grade_count_val = []\n",
    "        priors_val = []\n",
    "        for x in range(6):\n",
    "            grade_count = X_train[X_train['Grade'] == x].count()\n",
    "            grade_count_val = grade_count.values.tolist()\n",
    "            lis_grade_count_val.append(grade_count_val[x])\n",
    "        for y in range(len(lis_grade_count_val)):\n",
    "            priors_val.append(lis_grade_count_val[y]/sum(lis_grade_count_val))\n",
    "    \n",
    "        predict(X_test)\n",
    "        accuracy_list.append(evaluate(X_test))\n",
    "\n",
    "    return (accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23.077, 23.846, 26.154, 21.538, 30.233]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
